{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EynO4QM2xidh"
   },
   "outputs": [],
   "source": [
    "# Meng-install library yang dibutuhkan untuk eksperimen\n",
    "# transformers  : library Hugging Face untuk model Transformer (BERT, IndoBERT, MiniLM, dll)\n",
    "# datasets      : library Hugging Face untuk pengelolaan dataset dan integrasi dengan Trainer\n",
    "# scikit-learn  : library machine learning untuk evaluasi (accuracy, precision, recall, F1, ROC-AUC, K-Fold)\n",
    "# torch         : PyTorch, framework deep learning yang digunakan untuk training model dan GPU acceleration\n",
    "# -q            : quiet mode, agar output instalasi tidak terlalu panjang\n",
    "!pip install -q transformers datasets scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZD6FqqO_xrF0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Library bawaan Python untuk membaca dan menulis data berformat JSON\n",
    "# Digunakan untuk memuat dataset chatbot dari file .json\n",
    "\n",
    "import numpy as np\n",
    "# Library numerik untuk operasi array dan perhitungan matematis\n",
    "# Digunakan untuk softmax, mean, std, dan manipulasi array prediksi\n",
    "\n",
    "from pathlib import Path\n",
    "# Digunakan untuk menangani path file secara aman dan portable\n",
    "\n",
    "import os\n",
    "# Library bawaan Python untuk operasi sistem (cek folder, hapus file, dll.)\n",
    "# Digunakan saat menyimpan model terbaik dari cross-validation\n",
    "\n",
    "import torch\n",
    "# Framework deep learning PyTorch\n",
    "# Digunakan sebagai backend training model Transformer dan GPU acceleration\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# Modul fungsi neural network PyTorch\n",
    "# Digunakan untuk softmax saat inferensi (prediksi confidence)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Digunakan untuk melakukan Stratified K-Fold Cross-Validation\n",
    "# Stratified memastikan distribusi label di setiap fold tetap seimbang\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "# LabelEncoder: mengubah label intent (string) menjadi angka\n",
    "# label_binarize: mengubah label menjadi format one-hot untuk ROC-AUC multi-class\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "# accuracy_score                  : menghitung akurasi klasifikasi\n",
    "# precision_recall_fscore_support : menghitung precision, recall, dan F1-score\n",
    "# roc_auc_score                   : menghitung ROC-AUC untuk klasifikasi multi-kelas\n",
    "# confusion_matrix                : menghasilkan confusion matrix\n",
    "\n",
    "from datasets import Dataset\n",
    "# Digunakan untuk membuat dataset Hugging Face\n",
    "# Memudahkan tokenisasi batch dan integrasi dengan Trainer\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "# BertTokenizer                    : memuat tokenizer bert\n",
    "# AutoModelForSequenceClassification: model Transformer untuk klasifikasi intent\n",
    "# DataCollatorWithPadding          : melakukan padding dinamis per batch\n",
    "# TrainingArguments                : konfigurasi proses training\n",
    "# Trainer                          : API training tingkat tinggi dari Hugging Face\n",
    "# set_seed                         : mengatur seed agar eksperimen reproducible\n",
    "\n",
    "SEED = 42\n",
    "# Seed random untuk memastikan hasil eksperimen konsisten (reproducibility)\n",
    "\n",
    "K_FOLDS = 5\n",
    "# Jumlah fold untuk Stratified K-Fold Cross-Validation\n",
    "\n",
    "MODEL_NAME = \"indobenchmark/indobert-lite-base-p1\"\n",
    "# Nama model pretrained yang digunakan\n",
    "\n",
    "DATA_PATH = \"dataset_chatbot.json\"\n",
    "# Path menuju file dataset chatbot dalam format JSON\n",
    "\n",
    "BEST_DIR = \"./best_fold_model\"\n",
    "# Direktori untuk menyimpan model terbaik hasil cross-validation\n",
    "\n",
    "LR = 3e-5\n",
    "# Learning rate untuk optimizer AdamW\n",
    "# Nilai umum yang stabil untuk fine-tuning model Transformer\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "# Ukuran batch training\n",
    "# Disesuaikan agar muat di GPU dan tetap stabil\n",
    "\n",
    "EPOCHS = 30\n",
    "# Jumlah epoch training\n",
    "# Cukup besar untuk dataset kecil–menengah agar model konvergen\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "# Panjang maksimum token input\n",
    "# Kalimat lebih panjang akan dipotong (truncation)\n",
    "\n",
    "set_seed(SEED)\n",
    "# Mengatur seed untuk PyTorch, NumPy, dan library terkait\n",
    "# Bertujuan agar hasil training dapat direproduksi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdfVpNybya8P",
    "outputId": "73228512-93c7-478c-ee0b-ac4f7326d4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 2040\n",
      "Jumlah intent: 204\n",
      "Contoh: Bagaimana cara melihat data pribadi saya di sistem akademik Unpad? -> prosedur_akses_data_pribadi_mahasiswa\n",
      "num_labels: 204\n"
     ]
    }
   ],
   "source": [
    "# Membaca isi file dataset (JSON) dari path DATA_PATH\n",
    "# File dibaca sebagai teks (string) dengan encoding UTF-8\n",
    "# json.loads digunakan untuk mengubah string JSON menjadi objek Python (list/dict)\n",
    "raw = json.loads(Path(DATA_PATH).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Menyiapkan dua list kosong:\n",
    "# - texts   : untuk menyimpan pertanyaan (input model)\n",
    "# - intents : untuk menyimpan label intent (kelas)\n",
    "texts, intents = [], []\n",
    "\n",
    "# Melakukan iterasi untuk setiap data (baris) dalam dataset\n",
    "for row in raw:\n",
    "    # Mengambil field \"question\" dari JSON\n",
    "    # strip() menghapus spasi di awal dan akhir teks\n",
    "    q = (row.get(\"question\") or \"\").strip()\n",
    "\n",
    "    # Mengambil field \"intent\" dari JSON\n",
    "    it = (row.get(\"intent\") or \"\").strip()\n",
    "    # hanya data yang memiliki question DAN intent yang dimasukkan\n",
    "    if q and it:\n",
    "        texts.append(q)       # simpan pertanyaan ke list texts\n",
    "        intents.append(it)    # simpan intent ke list intents\n",
    "\n",
    "# Menampilkan jumlah total data valid yang digunakan\n",
    "print(\"Total data:\", len(texts))\n",
    "\n",
    "# Menampilkan jumlah intent unik dalam dataset\n",
    "# set(intents) digunakan untuk menghilangkan duplikasi label\n",
    "print(\"Jumlah intent:\", len(set(intents)))\n",
    "\n",
    "# Menampilkan satu contoh pasangan (question -> intent)\n",
    "# Berguna untuk sanity check apakah data sudah benar\n",
    "print(\"Contoh:\", texts[0], \"->\", intents[0])\n",
    "\n",
    "# Membuat objek LabelEncoder dari scikit-learn\n",
    "# Digunakan untuk mengubah label intent (string) menjadi angka\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Mengubah semua intent (string) menjadi label numerik (0, 1, 2, ...)\n",
    "y = le.fit_transform(intents)\n",
    "\n",
    "# Menghitung jumlah total kelas / intent\n",
    "# le.classes_ berisi daftar intent unik yang telah di-encode\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# Menampilkan jumlah kelas (akan digunakan saat inisialisasi model)\n",
    "print(\"num_labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "95149ee600ea4be980c193f0ed81bb21",
      "9e68b71078e74ac88e2cc9f8157bb5a2",
      "eafe1b844c744647a2d08e43ae788cf0",
      "f8cf923c041f4d249f0f3fbdf8a686b2",
      "8e153fcc24d54a8a9ad0ec2691e1f3cc",
      "ff993416b1a04e92955c691aca73e519",
      "71d7975335514457aa56ba61b676f386",
      "3bb6dea1ee2f427baac8d31072653fd4",
      "29536f0a0d2442a29c8bb54bd9fd7b3d",
      "d3ed8266e2724329979987dcb329e20c",
      "2f8817e0739e4e308131f982e37a51c5",
      "6b869b79d6624efca0c859ea8364f705",
      "c4089a19818d41c787069b03c3088894",
      "3d2e48cee52b4bf4bfa47d54ba5b3d07",
      "32f9f3c064224516860b524ba7b1f05c",
      "c4a847599dba42c39fa106411b90ebe2",
      "fc95676e6e1d4cdc89b11b6b6f2799bf",
      "3557ed9b16d547e0aa80933e1b75acac",
      "933b75d0341e4e669f2327d94c020b7b",
      "fbdf0289b06e4c24a6b94471cb4d0a72",
      "c00ae6c1eb1948c3bf2cbc415b336509",
      "961dbc4cf87446dc8ed043cb631a24a7",
      "f153b59376a04e349323bdc0db147a8a",
      "2085c0a33b574b93a803bee4a682fae9",
      "85bd7ef4e18947ff80422778e6074fa2",
      "8ec611131c784626b93de4ee26e9b2fd",
      "8ef8c36854224f4e98bddabdb64bd4b4",
      "cffc8c607d8b4c498a0ff41437136fba",
      "ad3eebd0fbab4d559e31f0bc7a1f8f9c",
      "3630340704bf4352ae0b2521ed6ba519",
      "398f316daaa743198acba3f0e4457c3a",
      "1b18b8935b0145ae881a4c596961ceb8",
      "5636c813e0c14d73a7cce22f16bc58cd",
      "c42c7118837f4b3186fc37f683161b32",
      "d0fa83a577e7496cb30e4ab10e753087",
      "c25fd3877359471cbffcbc141013e131",
      "7e35c7c3cf98460c84e5a8d202115b5f",
      "5a00af238c304b1bb319bf36368930d2",
      "c8ec4c2412734b6c88cd6fe41efd51a5",
      "5a67172178d949519ce4f60cd74660a1",
      "6784302a80f44f358bfd152eebc99021",
      "5d9aaff5036b470184c512267f5c2a10",
      "2ec5240ba87c4655b39a2cf3d2593e5e",
      "a2165ace2f934806a997db684234386c"
     ]
    },
    "id": "jM6FhpCGyq-v",
    "outputId": "ca109dfa-775f-49b4-f77d-0a4314554d7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Memuat bert tokenizer sesuai dengan model pretrained yang digunakan\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# DataCollatorWithPadding digunakan untuk melakukan padding secara dinamis\n",
    "# Padding dilakukan berdasarkan panjang teks terpanjang di setiap batch\n",
    "# Hal ini membuat training lebih efisien dibanding padding ke panjang maksimum\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengubah teks mentah menjadi token numerik\n",
    "    yang dapat diproses oleh model Transformer.\n",
    "\n",
    "    Parameter:\n",
    "    - batch[\"text\"]: kumpulan teks (pertanyaan) dalam satu batch\n",
    "\n",
    "    Proses:\n",
    "    - truncation=True  : memotong teks jika melebihi MAX_LENGTH\n",
    "    - max_length       : batas maksimum jumlah token\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "def softmax_np(logits):\n",
    "    \"\"\"\n",
    "    Menghitung softmax menggunakan NumPy.\n",
    "\n",
    "    Digunakan untuk mengubah nilai logit (skor mentah model)\n",
    "    menjadi probabilitas kelas.\n",
    "\n",
    "    logits: array dengan shape [n_samples, n_classes]\n",
    "    \"\"\"\n",
    "    # Mengurangi nilai maksimum tiap baris untuk stabilitas numerik\n",
    "    logits = logits - np.max(logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Menghitung eksponensial dari setiap nilai logit\n",
    "    exp = np.exp(logits)\n",
    "\n",
    "    # Membagi dengan total eksponensial untuk mendapatkan probabilitas\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def compute_all_metrics(y_true, y_pred, y_proba, num_labels):\n",
    "    \"\"\"\n",
    "    Menghitung berbagai metrik evaluasi untuk klasifikasi multi-kelas.\n",
    "\n",
    "    Parameter:\n",
    "    - y_true     : label asli (ground truth)\n",
    "    - y_pred     : label hasil prediksi model\n",
    "    - y_proba    : probabilitas hasil softmax\n",
    "    - num_labels : jumlah total kelas / intent\n",
    "    \"\"\"\n",
    "\n",
    "    # Menghitung akurasi klasifikasi\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Menghitung precision, recall, dan F1-score\n",
    "    # average=\"macro\" digunakan agar semua kelas diperlakukan setara\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Mengubah label asli menjadi representasi one-hot\n",
    "    # Diperlukan untuk perhitungan ROC-AUC multi-kelas\n",
    "    y_true_oh = label_binarize(\n",
    "        y_true,\n",
    "        classes=list(range(num_labels))\n",
    "    )\n",
    "\n",
    "    # Menghitung ROC-AUC dengan pendekatan One-vs-Rest (OvR)\n",
    "    # Try-except digunakan karena ROC-AUC bisa gagal jika\n",
    "    # suatu fold tidak mengandung semua kelas\n",
    "    try:\n",
    "        auc = roc_auc_score(\n",
    "            y_true_oh,\n",
    "            y_proba,\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"macro\"\n",
    "        )\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    # Menghitung confusion matrix\n",
    "    # Baris = label sebenarnya, kolom = label prediksi\n",
    "    cm = confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=list(range(num_labels))\n",
    "    )\n",
    "\n",
    "    # Mengembalikan seluruh metrik dalam bentuk dictionary\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec),\n",
    "        \"recall_macro\": float(rec),\n",
    "        \"f1_macro\": float(f1),\n",
    "        \"roc_auc_ovr_macro\": float(auc),\n",
    "        \"confusion_matrix\": cm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c50765bda8b841af9cbf65fadb79a568",
      "c61b0ef7bf854a429073bcefefcec184",
      "9b6df0a8b2c24648b8e85fbeb8943374",
      "c9a2ef2565a34ef6b6d58e70c11d5309",
      "5bb25cb88e58474ca40c9c99dd4e2f82",
      "0adc7c8f3a9145a6b2c09f219aac1171",
      "02f7b45f10764019b77f12f7672053d7",
      "6ff97a408b03427f8b1cb4b85e399c47",
      "5dd4edd7c92d4c8e845e0d27cbfaef10",
      "5c2fc305a1694af4b66c056bedab725b",
      "28eec8b16dc44608a353fd3931ca74cb",
      "ff8b0cbacaaf497cad8535dcb6b4dae1",
      "1f8e6965dcdb4689ad2ce524b2b5d5c7",
      "1b986ca5befe43adabfdc67ea4e70d2c",
      "2e3efc62e1ad43309ac13a11ff6c768d",
      "16512a15e68e43159ba2d0e7ba114237",
      "36153b1c4f5240019ce9b8c5ffcc992d",
      "fafee01a99564f68ac7d1a162414a69b",
      "291f582747e44e81b9f80be44a187c63",
      "be4b59cb15214b4d85243b7fe7bd9ef7",
      "fa4dbfc21f424669baebcf503a2a925c",
      "f59316c070ac46be98aa31c0d998fcd8",
      "256bad3a8cad4c14bf5b9ca782b333ac",
      "cf8550040b234d37931a0f794d4055e8",
      "6644e179bb9948bb99e189bcfc1b4510",
      "39c465b38cd840cd9c45f1fa62432638",
      "267889eb700c47a68e5f865ff7bef379",
      "edb7c3c8e7924b95b1a39bbb85f3dd69",
      "e396d565b77f47a08d7fd741f808c7ee",
      "73239c6b0cdd4d67881ca9bc448fffb6",
      "1906051af00e4d678ede8cd8fbefaa16",
      "90afbbdf7ae64ccfb242d821aad1e6c6",
      "d159cc80f6e640d2953c9c7ff873ab17",
      "f5c6ef833ae540e6b208cd80801e2eac",
      "2df076843e7943b68d3c484790af82ce",
      "fe59406a9dd84f5b8a10dcec560685e1",
      "861cf7fb9a114c92b8a3fc8ea4afe612",
      "b588e470ac4c4a05a661b35e5df40c4d",
      "f7a4ca8dc564445ea0d28b49e26f442f",
      "c240de7da3ce438e85a9f4a1072e7f14",
      "ff93a3d6a2fa4c8a8a664dbaadd1e826",
      "df1e810487f34f9fa2df64d8405b67ab",
      "77094f37045d4779930ce57c6341badd",
      "ea459e084cd744279e5adec2c1c64ce9",
      "f80a003ce3fa4f2eabc1f920a73262c2",
      "bbfe64875bf94790883f124f1268b1ad",
      "7532836b7f344135aea1feeb080d03a5",
      "1e43c098485e40bb961bb7071d62c2c0",
      "86ffcc26708a4b5b9b7887dd486b26db",
      "87aec6b4b9594c42a405f21b6437450f",
      "94120377936d49cea7354ce167bfbd96",
      "cbfba37cd3954ced9e5dffe374505b62",
      "888ec371be454f408186f27adefa230f",
      "a8340619115a4f42badf68782fa6592d",
      "b40a4aaf15f847bb928823a30bdc9fb6",
      "31c1f51892484e55b37b9c5ad15b2df3",
      "684c24909b4f44ec87c3ca8f048055f9",
      "322809dc3cad43498ac61f030a808f68",
      "c8e63878a1ba46f587239fcce90d453c",
      "b7e12f0b7e2447c5b0733c0a8aba2064",
      "1f014b81d6354969b6453dd26fbccb46",
      "f8769efb6d9343f09b7808897617e9b2",
      "affdd66f20ae41dabd75bd55dd64a753",
      "9159c75b308a476db41f561b1f6f304b",
      "9d86c9c40ce14233bfafb850f5efb3f1",
      "9b756e23167649268c1d4d17675a0cac",
      "7ed040b087bf49afb85125a6eb27a2a3",
      "b36879b2152846e5a770a73b8f2ffe65",
      "7bc020648e5e4310ba3c033b52006253",
      "52eaf760b6834dd1b81932938387e7bc",
      "3d43fd947c794acf819ebe175ccea897",
      "fdec8b78b63e47ba8c487bf0adb59796",
      "5a70ebf260054637ac81d8d9f6db2fb2",
      "5f671d4d2ee44ee5bc6e701c4b24d531",
      "6466fb26646048448312b576445bdb23",
      "9f70c9cc83494ea09438c5132eb18557",
      "ea533a909dad41eea67a6603cb9112b4",
      "ff4ce38b14e54651a075585e54ef8161",
      "13f07f932e654373ac4e7270f5123566",
      "17d48693a0f04ca3a47710edafe3d1d5",
      "eef2baf38edf42c8886bc4cb9bba0932",
      "0832b90051e34e2f92c87a5f9aca0b71",
      "8ad0c2dff4284b50b410525483eec942",
      "1095574229c64809a9161ebf413453e9",
      "0eba5a9a20c04e4bb52f74a940dc50a9",
      "e5901106bc8d472280a341a8102fe49d",
      "61f1b194feda4a52a823d8234744af91",
      "275fb983834446df9bcd9d40f209be19",
      "169e6ec752ed4abeb87bff758034ecca",
      "21c4a5c0749741fd8fc2e572fa6c2e8c",
      "19083cdad1f346c29bb1ff145543d5ea",
      "3317204ad99d48958a34952485872c27",
      "4aa719fdbdab4149929ab960d1db6fde",
      "00eb91be673b459a98b4d11844f78b74",
      "ad2aac9399e0490393cc825883f55a30",
      "30da338871f3468289dd0059a0e6ba9e",
      "03eed1fc763a4bb18032eb2d5ed5c2c7",
      "06cb08e01c21416a82a19dbc7b10efda",
      "0485d681c2b44a06bc3341bdb3c122db",
      "53919977011446e797a0e8a7ec509b8f",
      "47fdc89c3d904ca5b26bf0bfca351014",
      "a1e7b5754b164b4fa0040f47ccb29d35",
      "db43502fb43a4e219f7069732391f2ee",
      "dec9738a75c64d4198c2338c6d83dae7",
      "0b73dd7362984615b894b72678bd9c72",
      "fcac46931c034566bc9a6ab59f6e1548",
      "55e433631d5849bb89094d7f18cf9539",
      "e09421106b754443b09f6d920e8714ef",
      "6f783292dea64160b8c988071d74f058",
      "5fffb794631446e2b0c56c932d221473",
      "7c88997240734afeb1adb943e953bf95",
      "137cde17e9c946ff938d54a39650ec9b",
      "6b317283e46d44cc998890d2631784dd",
      "abd73d7d5e6e428295ba0361264fb17d",
      "b72fa50e72024748bd18528c97ceb6f2",
      "766196fb5f6b441ea423734b2ef36a36",
      "9b1599cfe67a489d9f288a24e0a9e3f4",
      "07dc04b11b444bba81f96f5f02aab038",
      "a3a21152e6c0456982dc37830673c99a",
      "1a1c8df8097244aeaf0ed2a0122ffce6",
      "fd4099b42ebe4f878c1ee5c205af5ce5",
      "06b44b119d934ed19fd61bf21f1f4e19",
      "935fa77d56ba48d4ac3e3a95033afcbd",
      "91a3a84d49ff484abfc2074fff5b0694",
      "a947b5970c2c4ababd90acd61bc4371b",
      "714e1181344f4634a8728d26e4002d8c",
      "2c6b314d92c94e56936630f6e5692ac3",
      "4e6eddcb74be4ef49b8e3394d48aa654",
      "16642cea73c140ebbf51c6b762aedf99",
      "5027becb713a47c192d1c5a4f5595e26",
      "27bdd6f58e684e3988bebc5b29dc6ce2",
      "6df7a01e52644620a7f5f3dcdc7edae3"
     ]
    },
    "id": "vLodwAUuzD7X",
    "outputId": "38e69407-7198-4278-9423-94035478f325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FOLD 1/5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3330309801.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 02:04, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold metrics: {'accuracy': 0.9264705882352942, 'precision_macro': 0.9411764705882353, 'recall_macro': 0.9264705882352942, 'f1_macro': 0.9205882352941177, 'roc_auc_ovr_macro': 0.9996377861489423, 'fold': 1}\n",
      "✅ Best model updated: fold=1, best_f1=0.9206 -> saved to ./best_fold_model\n",
      "\n",
      "==================== FOLD 2/5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3330309801.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 01:53, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold metrics: {'accuracy': 0.9313725490196079, 'precision_macro': 0.94281045751634, 'recall_macro': 0.9313725490196079, 'f1_macro': 0.9276143790849674, 'roc_auc_ovr_macro': 0.999740413406742, 'fold': 2}\n",
      "✅ Best model updated: fold=2, best_f1=0.9276 -> saved to ./best_fold_model\n",
      "\n",
      "==================== FOLD 3/5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3330309801.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 01:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold metrics: {'accuracy': 0.8700980392156863, 'precision_macro': 0.8912581699346405, 'recall_macro': 0.8700980392156863, 'f1_macro': 0.8626050420168068, 'roc_auc_ovr_macro': 0.9982070414372646, 'fold': 3}\n",
      "\n",
      "==================== FOLD 4/5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3330309801.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 01:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.861600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold metrics: {'accuracy': 0.9142156862745098, 'precision_macro': 0.9272875816993466, 'recall_macro': 0.9142156862745098, 'f1_macro': 0.9058823529411765, 'roc_auc_ovr_macro': 0.9987081039312277, 'fold': 4}\n",
      "\n",
      "==================== FOLD 5/5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3330309801.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 01:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold metrics: {'accuracy': 0.9019607843137255, 'precision_macro': 0.9006535947712418, 'recall_macro': 0.9019607843137255, 'f1_macro': 0.8876050420168068, 'roc_auc_ovr_macro': 0.9993661257606491, 'fold': 5}\n",
      "\n",
      "==================== BEST FOLD ====================\n",
      "Best fold: 2\n",
      "Best metrics: {'accuracy': 0.9313725490196079, 'precision_macro': 0.94281045751634, 'recall_macro': 0.9313725490196079, 'f1_macro': 0.9276143790849674, 'roc_auc_ovr_macro': 0.999740413406742, 'fold': 2}\n"
     ]
    }
   ],
   "source": [
    "# Membuat objek StratifiedKFold\n",
    "# n_splits=K_FOLDS   : jumlah fold (misalnya 5-fold)\n",
    "# shuffle=True       : data diacak sebelum dibagi\n",
    "# random_state=SEED  : agar pembagian data konsisten (reproducible)\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# List untuk menyimpan hasil evaluasi tiap fold (accuracy, precision, recall, f1, dll)\n",
    "fold_results = []\n",
    "\n",
    "# List untuk menyimpan confusion matrix dari tiap fold\n",
    "conf_mats = []\n",
    "\n",
    "# Variabel untuk menyimpan informasi fold terbaik\n",
    "best_fold = None            # nomor fold terbaik\n",
    "best_score = -1.0           # skor terbaik (pakai F1-macro)\n",
    "best_metrics = None         # metrik dari fold terbaik\n",
    "\n",
    "# Mengecek apakah folder BEST_DIR sudah ada\n",
    "if os.path.isdir(BEST_DIR):\n",
    "    # Menghapus seluruh isi folder BEST_DIR\n",
    "    # Ini dilakukan agar model lama tidak tertimpa atau tercampur\n",
    "    for root, dirs, files in os.walk(BEST_DIR, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))  # hapus file\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))   # hapus subfolder\n",
    "    os.rmdir(BEST_DIR)  # hapus folder utama\n",
    "\n",
    "# Looping untuk setiap fold\n",
    "# skf.split(texts, y) akan menghasilkan index data train dan validation\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, y), start=1):\n",
    "\n",
    "    print(f\"\\n==================== FOLD {fold}/{K_FOLDS} ====================\")\n",
    "\n",
    "    # Menyusun data training berdasarkan index train_idx\n",
    "    train_data = {\n",
    "        \"text\":  [texts[i] for i in train_idx],  # pertanyaan untuk training\n",
    "        \"label\": [int(y[i]) for i in train_idx]  # label intent untuk training\n",
    "    }\n",
    "\n",
    "    # Menyusun data validasi berdasarkan index val_idx\n",
    "    val_data = {\n",
    "        \"text\":  [texts[i] for i in val_idx],     # pertanyaan untuk validasi\n",
    "        \"label\": [int(y[i]) for i in val_idx]     # label intent untuk validasi\n",
    "    }\n",
    "\n",
    "    # Mengubah dictionary menjadi Hugging Face Dataset\n",
    "    # lalu melakukan tokenisasi secara batch\n",
    "    ds_train = Dataset.from_dict(train_data).map(tokenize_fn, batched=True)\n",
    "    ds_val   = Dataset.from_dict(val_data).map(tokenize_fn, batched=True)\n",
    "\n",
    "    # Menghapus kolom \"text\" karena model hanya butuh input_ids & attention_mask\n",
    "    # with_format(\"torch\") agar output berupa tensor PyTorch\n",
    "    ds_train = ds_train.remove_columns([\"text\"]).with_format(\"torch\")\n",
    "    ds_val   = ds_val.remove_columns([\"text\"]).with_format(\"torch\")\n",
    "\n",
    "    # Model selalu dibuat ulang di setiap fold\n",
    "    # Ini penting agar tidak terjadi kebocoran informasi antar fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    # Konfigurasi training\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./cv_runs/fold_{fold}\",   # folder output khusus fold ini\n",
    "        learning_rate=LR,                      # learning rate\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,               # jumlah epoch\n",
    "        weight_decay=0.01,                     # regularisasi\n",
    "        logging_steps=50,                      # interval logging\n",
    "        report_to=\"none\",                      # tidak pakai wandb/tensorboard\n",
    "        seed=SEED,\n",
    "        fp16=torch.cuda.is_available(),        # pakai FP16 jika ada GPU\n",
    "    )\n",
    "\n",
    "    # Membuat Trainer (API training Hugging Face)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_train,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # TRAINING MODEL PADA FOLD INI\n",
    "    trainer.train()\n",
    "\n",
    "    # Melakukan prediksi pada data validasi\n",
    "    out = trainer.predict(ds_val)\n",
    "\n",
    "    # Logits = skor mentah model (belum softmax)\n",
    "    logits = out.predictions\n",
    "\n",
    "    # Label asli (ground truth)\n",
    "    y_true = out.label_ids.astype(int)\n",
    "\n",
    "    # Mengubah logits menjadi probabilitas\n",
    "    y_proba = softmax_np(logits)\n",
    "\n",
    "    # Mengambil kelas dengan probabilitas tertinggi\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "    # Menghitung seluruh metrik evaluasi\n",
    "    metrics = compute_all_metrics(y_true, y_pred, y_proba, num_labels)\n",
    "\n",
    "    # Mengambil confusion matrix\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "\n",
    "    # Menyimpan metrik (tanpa confusion matrix agar ringkas)\n",
    "    fold_row = {k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "    fold_row[\"fold\"] = fold\n",
    "\n",
    "    fold_results.append(fold_row)\n",
    "    conf_mats.append(cm)\n",
    "\n",
    "    print(\"Fold metrics:\", fold_row)\n",
    "\n",
    "    # ======================\n",
    "    # MENENTUKAN FOLD TERBAIK\n",
    "    # ======================\n",
    "\n",
    "    # Jika F1-macro fold ini lebih baik dari sebelumnya\n",
    "    if fold_row[\"f1_macro\"] > best_score:\n",
    "        best_score = fold_row[\"f1_macro\"]\n",
    "        best_fold = fold\n",
    "        best_metrics = fold_row\n",
    "\n",
    "        # Membuat folder untuk menyimpan model terbaik\n",
    "        os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "        # Menyimpan model dan tokenizer dari fold terbaik\n",
    "        trainer.save_model(BEST_DIR)\n",
    "        tokenizer.save_pretrained(BEST_DIR)\n",
    "\n",
    "        # Menyimpan mapping label index -> intent\n",
    "        with open(os.path.join(BEST_DIR, \"label_names.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(le.classes_.tolist(), f, ensure_ascii=False, indent=2)\n",
    "        print(\n",
    "            f\"✅ Best model updated: \"\n",
    "            f\"fold={best_fold}, best_f1={best_score:.4f} -> saved to {BEST_DIR}\"\n",
    "        )\n",
    "\n",
    "# ======================\n",
    "# RINGKASAN FOLD TERBAIK\n",
    "# ======================\n",
    "\n",
    "print(\"\\n==================== BEST FOLD ====================\")\n",
    "print(\"Best fold:\", best_fold)\n",
    "print(\"Best metrics:\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snrKoqipzFOC",
    "outputId": "70f0e7d8-ed74-44e2-d69c-ac6ac10d6771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CV SUMMARY (K=5) ====================\n",
      "Accuracy           : 0.9088 ± 0.0219\n",
      "Precision (macro)  : 0.9206 ± 0.0211\n",
      "Recall (macro)     : 0.9088 ± 0.0219\n",
      "F1-score (macro)   : 0.9009 ± 0.0235\n",
      "ROC-AUC (OvR macro): 0.9991 ± 0.0006\n",
      "\n",
      "Confusion matrix rata-rata shape: (204, 204)\n",
      "\n",
      "Detail per fold:\n",
      "{'accuracy': 0.9264705882352942, 'precision_macro': 0.9411764705882353, 'recall_macro': 0.9264705882352942, 'f1_macro': 0.9205882352941177, 'roc_auc_ovr_macro': 0.9996377861489423, 'fold': 1}\n",
      "{'accuracy': 0.9313725490196079, 'precision_macro': 0.94281045751634, 'recall_macro': 0.9313725490196079, 'f1_macro': 0.9276143790849674, 'roc_auc_ovr_macro': 0.999740413406742, 'fold': 2}\n",
      "{'accuracy': 0.8700980392156863, 'precision_macro': 0.8912581699346405, 'recall_macro': 0.8700980392156863, 'f1_macro': 0.8626050420168068, 'roc_auc_ovr_macro': 0.9982070414372646, 'fold': 3}\n",
      "{'accuracy': 0.9142156862745098, 'precision_macro': 0.9272875816993466, 'recall_macro': 0.9142156862745098, 'f1_macro': 0.9058823529411765, 'roc_auc_ovr_macro': 0.9987081039312277, 'fold': 4}\n",
      "{'accuracy': 0.9019607843137255, 'precision_macro': 0.9006535947712418, 'recall_macro': 0.9019607843137255, 'f1_macro': 0.8876050420168068, 'roc_auc_ovr_macro': 0.9993661257606491, 'fold': 5}\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghitung rata-rata dan standar deviasi\n",
    "# np.nanmean / np.nanstd dipakai agar nilai NaN (misalnya dari ROC-AUC)\n",
    "# tidak menyebabkan error pada perhitungan\n",
    "def mean_std(xs):\n",
    "    return float(np.nanmean(xs)), float(np.nanstd(xs))\n",
    "\n",
    "# Mengambil nilai accuracy, precision, recall, dll dari setiap fold\n",
    "accs  = [r[\"accuracy\"] for r in fold_results]\n",
    "precs = [r[\"precision_macro\"] for r in fold_results]\n",
    "recs  = [r[\"recall_macro\"] for r in fold_results]\n",
    "f1s   = [r[\"f1_macro\"] for r in fold_results]\n",
    "aucs  = [r[\"roc_auc_ovr_macro\"] for r in fold_results]\n",
    "\n",
    "# Menampilkan rata-rata ± standar deviasi untuk setiap metrik\n",
    "print(\"\\n==================== CV SUMMARY (K=5) ====================\")\n",
    "m, s = mean_std(accs);  print(f\"Accuracy           : {m:.4f} ± {s:.4f}\")\n",
    "m, s = mean_std(precs); print(f\"Precision (macro)  : {m:.4f} ± {s:.4f}\")\n",
    "m, s = mean_std(recs);  print(f\"Recall (macro)     : {m:.4f} ± {s:.4f}\")\n",
    "m, s = mean_std(f1s);   print(f\"F1-score (macro)   : {m:.4f} ± {s:.4f}\")\n",
    "m, s = mean_std(aucs);  print(f\"ROC-AUC (OvR macro): {m:.4f} ± {s:.4f}\")\n",
    "\n",
    "# Menghitung confusion matrix rata-rata dari seluruh fold\n",
    "cm_avg = np.mean(np.stack(conf_mats), axis=0)\n",
    "print(\"\\nConfusion matrix rata-rata shape:\", cm_avg.shape)\n",
    "\n",
    "# Menampilkan detail hasil evaluasi untuk setiap fold\n",
    "print(\"\\nDetail per fold:\")\n",
    "for r in fold_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "o2hVmbpHzJ9b",
    "outputId": "bae28dd1-eaf4-4152-8ad6-a21171bdddb2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Import library matplotlib untuk visualisasi data (plot / grafik)\n",
    "\n",
    "# Membuat figure (kanvas) dengan ukuran 10 x 8 inci\n",
    "# Ukuran ini dipilih agar confusion matrix terlihat jelas\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Menampilkan confusion matrix rata-rata sebagai gambar (heatmap)\n",
    "# cm_avg berisi nilai confusion matrix hasil rata-rata 5-fold CV\n",
    "# aspect=\"auto\" agar skala sumbu menyesuaikan ukuran matriks\n",
    "plt.imshow(cm_avg, aspect=\"auto\")\n",
    "\n",
    "# Menampilkan colorbar di samping gambar\n",
    "# Colorbar menunjukkan intensitas warna (besar-kecilnya nilai)\n",
    "plt.colorbar()\n",
    "\n",
    "# Judul plot\n",
    "plt.title(\"Confusion Matrix Rata-rata (5-Fold CV)\")\n",
    "# Label sumbu X (kolom) = kelas hasil prediksi model\n",
    "plt.xlabel(\"Predicted\")\n",
    "# Label sumbu Y (baris) = kelas label sebenarnya (ground truth)\n",
    "plt.ylabel(\"True\")\n",
    "# Mengatur layout agar tidak ada teks yang terpotong\n",
    "plt.tight_layout()\n",
    "# Menampilkan plot ke layar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWddi8CuzNGG",
    "outputId": "025f02c0-10c9-4207-bc10-d92d4f6c268c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Cara pinjam ruangan PPBS?\n",
      " - prosedur_peminjaman_ruangan_ppbs: 0.9639\n",
      " - prosedur_ganti_password_lms: 0.0013\n",
      " - prosedur_pendaftaran_beasiswa_pacis: 0.0011\n",
      " - prosedur_pendaftaran_beasiswa: 0.0010\n",
      " - info_metode_pembayaran_ukt: 0.0008\n",
      "\n",
      "Q: Cara isi KRS?\n",
      " - prosedur_pengisian_krs: 0.7640\n",
      " - info_jadwal_pengisian_krs: 0.1035\n",
      " - konsekuensi_keterlambatan_pengisian_krs: 0.0397\n",
      " - info_perubahan_krs: 0.0108\n",
      " - prosedur_upload_tugas_lms: 0.0026\n",
      "\n",
      "Q: Gimana alur skripsi dari awal sampai selesai?\n",
      " - info_prosedur_pelaksanaan_skripsi_sampai_wisuda: 0.9742\n",
      " - alur_pengajuan_dan_pelaksanaan_program_magang_hingga_selesai: 0.0007\n",
      " - info_penyampaian_hasil_evaluasi_ke_dosen: 0.0005\n",
      " - prosedur_pengajuan_sidang_skripsi: 0.0005\n",
      " - info_syarat_umum_kelulusan: 0.0004\n",
      "\n",
      "Q: Cara isi absen matkul?\n",
      " - prosedur_absensi_kuliah_pacis: 0.9717\n",
      " - info_jadwal_kuliah: 0.0015\n",
      " - info_rekap_kehadiran_mahasiswa: 0.0010\n",
      " - info_keterlambatan_pengumpulan_tugas_lms: 0.0006\n",
      " - info_penyelenggaraan_praktikum: 0.0006\n",
      "\n",
      "Q: Apakah Bu Mira ada di kampus hari ini?\n",
      " - info_keberadaan_dosen_di_kampus: 0.7876\n",
      " - info_profil_lengkap_dosen: 0.0127\n",
      " - info_media_unggah_materi_perkuliahan: 0.0088\n",
      " - info_media_pengumuman_kuliah_oleh_dosen: 0.0066\n",
      " - info_riwayat_studi_mahasiswa: 0.0059\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# Import tokenizer dan model untuk inference (deploy)\n",
    "\n",
    "# Menentukan device:\n",
    "# - GPU (cuda) jika tersedia\n",
    "# - CPU jika tidak ada GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Memuat tokenizer yang disimpan dari fold terbaik\n",
    "# Tokenizer ini HARUS sama dengan tokenizer saat training\n",
    "deploy_tokenizer = AutoTokenizer.from_pretrained(BEST_DIR)\n",
    "\n",
    "# Memuat model klasifikasi intent dari folder BEST_DIR\n",
    "deploy_model = AutoModelForSequenceClassification.from_pretrained(BEST_DIR).to(device)\n",
    "\n",
    "# Mengatur model ke mode evaluasi\n",
    "# agar:\n",
    "# - dropout dimatikan\n",
    "# - hasil prediksi konsisten\n",
    "deploy_model.eval()\n",
    "\n",
    "# Membaca mapping index → nama intent\n",
    "# File ini disimpan saat training\n",
    "with open(os.path.join(BEST_DIR, \"label_names.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    label_names = json.load(f)\n",
    "\n",
    "\n",
    "def predict_topk(text, top_k=5):\n",
    "    \"\"\"\n",
    "    Melakukan prediksi intent untuk satu input teks\n",
    "    dan mengembalikan top-k intent dengan probabilitas tertinggi\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenisasi input teks\n",
    "    # return_tensors=\"pt\" -> output berupa tensor PyTorch\n",
    "    inputs = deploy_tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Memindahkan input ke device yang sama dengan model (CPU/GPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Menonaktifkan perhitungan gradien (hemat memori & lebih cepat)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass model\n",
    "        # logits = skor mentah untuk setiap intent\n",
    "        logits = deploy_model(**inputs).logits\n",
    "\n",
    "    # Mengubah logits menjadi probabilitas menggunakan softmax\n",
    "    # squeeze(0) menghilangkan dimensi batch -> shape jadi [num_classes]\n",
    "    probs = F.softmax(logits, dim=1).squeeze(0)\n",
    "\n",
    "    # Mengambil top-k probabilitas terbesar\n",
    "    # vals = nilai probabilitas\n",
    "    # idxs = index kelas\n",
    "    vals, idxs = torch.topk(probs, k=top_k)\n",
    "\n",
    "    # Mengembalikan hasil dalam bentuk:\n",
    "    # (nama_intent, probabilitas)\n",
    "    return [(label_names[i], float(v)) for v, i in zip(vals.tolist(), idxs.tolist())]\n",
    "\n",
    "\n",
    "# Contoh pertanyaan untuk pengujian chatbot\n",
    "tests = [\n",
    "    \"Cara pinjam ruangan PPBS?\",\n",
    "    \"Cara isi KRS?\",\n",
    "    \"Gimana alur skripsi dari awal sampai selesai?\",\n",
    "    \"Cara isi absen matkul?\",\n",
    "    \"Apakah Bu Mira ada di kampus hari ini?\"\n",
    "]\n",
    "\n",
    "# Melakukan prediksi top-5 intent untuk setiap pertanyaan\n",
    "for t in tests:\n",
    "    print(\"\\nQ:\", t)\n",
    "    for tag, p in predict_topk(t, top_k=5):\n",
    "        print(f\" - {tag}: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Ykyg2_fwi6-D",
    "outputId": "35b19b4c-dc2f-45e7-c68d-300ad1fc9f3e"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_61a6eaee-82ea-4a30-a65f-5e7e071cccdc\", \"best_fold_model.zip\", 44089267)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\n",
    "    \"best_fold_model\",  # nama file zip\n",
    "    \"zip\",              # format zip\n",
    "    BEST_DIR             # folder yang mau di-zip\n",
    ")\n",
    "from google.colab import files\n",
    "files.download(\"best_fold_model.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
